### ğŸ§  **Building a Neural Network from Scratch for MNIST Classification**  
This project implements a **Neural Network (NN) from scratch using NumPy** to classify handwritten digits from the **MNIST dataset**. Instead of using deep learning frameworks like TensorFlow or PyTorch, this project focuses on **understanding the fundamentals of neural networks**, including:  

âœ… **Forward Propagation** â€“ Computing activations and predictions  
âœ… **Backpropagation** â€“ Updating weights using gradient descent  
âœ… **Activation Functions** â€“ ReLU and Softmax for classification  
âœ… **Loss Function** â€“ Cross-Entropy for multi-class classification  
âœ… **Weight Initialization & Regularization**  

By building this model from scratch, this project demonstrates **a strong understanding of deep learning principles**, essential for **ML & AI roles at top tech companies.**  

---

## ğŸ¯ **Key Objectives**  
âœ” **Implement a fully connected Neural Network from scratch**  
âœ” **Train on the MNIST dataset without deep learning libraries**  
âœ” **Understand the math behind backpropagation & gradient updates**  
âœ” **Compare performance with modern deep learning frameworks**  

---

## ğŸ“Š **Dataset Overview: MNIST Handwritten Digits**  
The MNIST dataset consists of **28x28 grayscale images** of handwritten digits (0-9), used as a **benchmark for image classification models**.  

ğŸ”¹ **60,000 training images**  
ğŸ”¹ **10,000 test images**  
ğŸ”¹ **Each image is a 784-dimensional vector (flattened 28x28 pixels)**  
ğŸ”¹ **Labels range from 0 to 9 (multi-class classification)**  

âœ… **Example: Visualizing MNIST Digits**  
```python
import matplotlib.pyplot as plt

plt.imshow(X_train[:, 0].reshape(28, 28), cmap="gray")
plt.title(f"Label: {Y_train[0]}")
plt.show()
```

---

## ğŸ— **Neural Network Architecture**  
This NN has **two layers**:  

| **Layer** | **Units** | **Activation** | **Purpose** |
|-----------|----------|----------------|-------------|
| **Input Layer** | 784 | â€” | Pixels from MNIST images |
| **Hidden Layer** | 10 | **ReLU** | Captures patterns & features |
| **Output Layer** | 10 | **Softmax** | Multi-class classification |

âœ… **Example: Forward Propagation Implementation**  
```python
def relu(Z):
    return np.maximum(0, Z)

def softmax(Z):
    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))
    return expZ / np.sum(expZ, axis=0, keepdims=True)

# Forward pass through the network
Z1 = np.dot(W1, X_train) + b1
A1 = relu(Z1)

Z2 = np.dot(W2, A1) + b2
A2 = softmax(Z2)  # Final probabilities
```

ğŸ’¡ **Why ReLU?**  
- Helps model **complex patterns** by introducing non-linearity.  
- Prevents **vanishing gradients**, unlike sigmoid/tanh.  

ğŸ’¡ **Why Softmax?**  
- Converts raw logits into **probabilities** for multi-class classification.  

---

## ğŸ”¥ **Training the Neural Network with Backpropagation**  
The model **updates weights iteratively** using **gradient descent** to minimize cross-entropy loss.  

âœ… **Example: Backpropagation Implementation**  
```python
def compute_gradients(X, Y, A1, A2, W2):
    m = X.shape[1]  # Number of examples
    dZ2 = A2 - Y  # Gradient of output layer
    dW2 = (1/m) * np.dot(dZ2, A1.T)
    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)

    dZ1 = np.dot(W2.T, dZ2) * (A1 > 0)  # ReLU derivative
    dW1 = (1/m) * np.dot(dZ1, X.T)
    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)

    return dW1, db1, dW2, db2
```
ğŸ’¡ **Key Learnings from Backpropagation:**  
âœ” **Gradient descent adjusts weights** by computing **partial derivatives of the loss function**.  
âœ” **ReLU's derivative** is **1 for positive values, 0 for negatives** (solves vanishing gradient problem).  
âœ” **Softmax + Cross-Entropy combination** ensures proper probability calibration.  

---

## ğŸ“ˆ **Model Performance & Accuracy**  
The neural network achieves:  
âœ” **85% accuracy on training set**  
âœ” **84% accuracy on validation set**  

âœ… **Example: Evaluating the Model**  
```python
def compute_accuracy(predictions, labels):
    return np.mean(predictions == labels) * 100

predictions = np.argmax(A2, axis=0)
accuracy = compute_accuracy(predictions, Y_train)
print(f"Training Accuracy: {accuracy:.2f}%")
```

---

## ğŸ“Š **Comparison with Deep Learning Frameworks**  
How does a **scratch-built NN** compare with **TensorFlow/PyTorch implementations?**  

| **Model** | **Training Accuracy** | **Validation Accuracy** | **Notes** |
|-----------|----------------------|------------------------|-----------|
| **Scratch Neural Network (NumPy)** | **85%** | **84%** | Fully custom implementation |
| **TensorFlow/Keras NN** | **98%** | **97%** | Uses optimizations like Adam |
| **PyTorch CNN** | **99%** | **98%** | Uses convolutional layers |

ğŸ’¡ **Observations:**  
- **Scratch NN performs well** but lacks modern optimizations like **Adam optimizer & dropout**.  
- **Deep learning frameworks** provide **higher accuracy & efficiency** with minimal effort.  

---

## ğŸ”® **Future Enhancements**  
ğŸ”¹ **Add Dropout Layers** â€“ To prevent overfitting  
ğŸ”¹ **Implement Mini-Batch Gradient Descent** â€“ For efficient training  
ğŸ”¹ **Extend to Convolutional Neural Networks (CNNs)** â€“ To improve performance  
ğŸ”¹ **Hyperparameter Optimization** â€“ Tune learning rate, batch size, and weight initialization  

---

## ğŸ¯ **Why This Project Stands Out for ML & AI Roles**  
âœ” **Demonstrates Deep Learning Fundamentals** â€“ No libraries, just NumPy!  
âœ” **Hands-on Understanding of Forward & Backpropagation**  
âœ” **Strong Math & Algorithmic Approach to ML**  
âœ” **Foundation for Building Custom Deep Learning Architectures**  

---

## ğŸ›  **How to Run This Project**  
1ï¸âƒ£ Clone the repo:  
   ```bash
   git clone https://github.com/shrunalisalian/nn-from-scratch-mnist.git
   ```
2ï¸âƒ£ Install dependencies:  
   ```bash
   pip install -r requirements.txt
   ```
3ï¸âƒ£ Run the Jupyter Notebook:  
   ```bash
   jupyter notebook nn-scratch-mnist.ipynb
   ```

---

Referance: https://www.youtube.com/watch?v=w8yWXqWQYmU&ab_channel=SamsonZhang

---

## ğŸ“Œ **Connect with Me**  
- **LinkedIn:** [Shrunali Salian](https://www.linkedin.com/in/shrunali-salian/)  
- **Portfolio:** [Your Portfolio Link](#)  
- **Email:** [Your Email](#)  
